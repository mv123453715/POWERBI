
 _____________ *Corresponding author E-mail address: anibrighty@gmail.com Received October 8, 2020 61 

 Available online at http://scik.org 
J. Math. Comput. Sci. 11 (2021), No. 1, 61-73 
https://doi.org/10.28919/jmcs/5094 
ISSN: 1927-5307 
SPARSE REPRESENTATION USING COMPRESSED SENSING VIA DEEP LEARNING 
D.M. ANNIE BRIGHTY CHRISTLIN1,*, M. SAFISH MARY2 
1Department of Computer Science, Manonmaniam Sundaranar University, Tamilnadu, India 
2Department of Computer Science, St Xaviers College, Tamilnadu, India 
Copyright © 2021 the author(s). This is an open access article distributed under the Creative Commons Attribution License,which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. 
Abstract: Since the patients are not static during the MRI acquisition, the image formation process can create some artifacts that could reduce the photograph quality. The Compressed Sensing (CS) mechanism is hired for reconstructing the unique picture from the given sparse data. Accordingly, CS can be applied to reduce the acceleration time for an MRI test considering the patient's health. So the sensing process is carried out by way of a projection matrix for reconstructing the sparse signals from a few numbers of measurements. The CS guarantees the healing of an authentic picture with an excessive possibility based totally on random gaussian projection matrices. However, sparse ternaries projections (Latin word)-1, zero, +1 are more apt for hardware implementation. The proposed deep learning technique is hired in this article to acquire very CS sparse ternary projections. The STDAENN architecture incorporates the sensing layer for the projection matrix and a reconstruction layer for non-linear sparse matrix continuously by auto-encoder. The overall performance of the proposed STDAENN method is compared with the present strategies primarily based on the imply top signal-to-noise ratio (PSNR) to check the picture nicely. 
Keywords: compressed sensing; deep learning; sparse ternarius (Latin word) projections. 
2010 AMS Subject Classification: 65Y04. 62 D.M. ANNIE BRIGHTY CHRISTLIN, M. SAFISH MARY 

1. INTRODUCTION 
CS merges the process of compression and sensing or acquisition. One of the properties such that sparsity is used to regain the signal has sampled at an appreciable decrease rate than the necessities of the Shanon/Nyquist theorem [1]. In the scientific regions such as MRI and CT, audio or video or picture [2], the effective processing and analysis of excessive-dimensional facts the usage of CS is very important. Consider the compressed signal x ϵ Rn is held by CS sensing mechanism. It is examined with the help of projection matrix. So the linear measurements are given below: 
l1= l2= … lm= l=Φx (1) 
Where Φ ϵ Rmxn, Here m x n is the projection matrix and l ϵ Rm is the vector. In Sparse Sampling, anticipate that either x is a sparse signal or that x has a sparse representation for a suitable basis rely the performance coefficients of the received signal for getting the under-determined linear system 
l = Φ ψ u (2) 
The l is obtained through the sparse vector. This is referred to as the solution of the l1minimization problem. 
min||u||1 subject to l = Φ ψ u (3) 
u ϵ Rn 
The predictable CS method basis pursuit [3] provides the time complexity of the n measurements o(s log (n/s)) within the n-dimensional-sparse signal based on random Gaussian matrices[4]. One important trouble that random matrices are normally considered to construct hardware is very tough. Additionally, the arbitrary matrices are treated with sign vectors of high dimension. If there is no rapid matrix multiplication algorithm, Deep Learning knowledge may be used to examine the couple of levels of statistics representation in image processing effectively. The known Deep Learning method has been used for image tremendous-resolution [5], CS [7][8]and image denoising [6]. 
In this article, the deep learning method is utilized to find out an optimized projection matrix and non-linear reconstruction plotting from obtained measurements to the original signal 63 SPARSE REPRESENTATION USING COMPRESSED SENSING VIA DEEP LEARNING 

for implementing the hardware by an apt projection. The proposed network architecture are implementing the hardware through the sparsity and binary constraints on required by sparse s{0,-1,+1} to be focused. The image patches are tested by the proposed STDAENN network and the images are acquired in a block-based manner using the learned projection matrix. The result of the STDAENN method shows that the image with high quality in the reconstruction can be achieved with 0.05 projections nonzero {-1, +1} entries experimentally. 
The review of Compressed Sensing is discussed in Sec 2. The Compressive Sensing via Deep learning is highlighted in Sec 3. The experimental results are analyzed in Section 4 for describing the improved CS approach in image reconstruction. Hence The STDAENN method is focused to improve the image quality. 
2. LITERATURE REVIEW 
2.1 Compressed Sensing via Deep Learning 
The healing for regaining the authentic sign from compressed sensing length is carried out with the aid of deep learning [7][8]. The Stacked Denoising Autoencoder (SDA) is used for plotting a map of non-linear sensing operator and reconstruction [7]. The block-based CS completely linear sensing and non-linear reconstruction method are carried out using this absolutely-related community via Deep Learning. In the training time, the sensing matrix and the reconstruction methods are optimized to offer the final result of the proposed method primarily based at the wonderful reconstruction and evaluation time [8]. Here, the projections are dealt with a dense matrix [7] [8]. So the projection matrix handled as a sparse matrix element with the range of -1, 0, +1. So it offers the fast computation at some stage in acquisition and also improves the hardware implementation. 
While a DNN is mapped with binary weights -1, +1, this binary connect technique is used in the time of the forward and backward propagations. So the collected gradients of the stored weights are regained [13]. 
At runtime, the formation process reduces the memory length. However, a few scaling 64 D.M. ANNIE BRIGHTY CHRISTLIN, M. SAFISH MARY 

elements are introduced for compensating the lack of introducing the burden binarization using BNN [14] and additionally this technique is used to compress the pre-retained community [15]. The proposed approach [16] is used for analyzing the binary weights, links and also bringing a carefully related community. This mechanism is elevated [17] for compressing DNN by connection pruning, Huffman coding, and weight quantization. 
The proposed sparse technique is used for imposing the sensing layer by the binarization method that yields a fairly sparse ternarius (Latin word) projection matrix. So the learned projections allowed speedy computations during acquisition. So this method is apt for hardware implementation. The primary layer is constructed for sensing the signal in this approaching network, which corresponds to the linear projection matrix and allows the reconstruction module to be the non-linear for obtaining an excessive overall performance. 
3. CS VIA DEEP LEARNING 
Now an efficient technique of CS is proposed for the clinical picture particularly STDAENN (Stacked Denoising Autoencoder Neural Network), applied to map the sensing and reconstruction layer for the non-linear measurements for acquiring an excessive overall performance. This phase describes the STDAENN architecture, observed by employing the stairs for the education set of rules 
3.1 STDAENN Architecture 
The STDAENN architecture is hired in fig. 1. It carries a sensing and reconstruction layer. The sensing layer (SeLa linear layer) presents in the left-half of the STDAENN architecture and the reconstruction layer (non-linear layer) presents in the right-hand side. The community takes an input as the vectorized picture patches (IM) of length n = S2. The sensing layer sends the n-dimensional input signal x to the m-dimensional domain. 
Thus, the variety of gadgets in this layer is m =s2v, in which the sensing value v presents between 0 and 1. The weights of sensing layer Θabx ϵ -1, 0, +1 m x n, which corresponds to the projection matrix Θabx= Φt. 65 SPARSE REPRESENTATION USING COMPRESSED SENSING VIA DEEP LEARNING 

In the reconstruction module, the scaling layer has the input as the output of the sensing layer by way of the found out factors α linearly. This residue consists of “m” hidden units that are linked “one-to-one” for “m” hidden units of the sensing layer and it's far used to make amends for the loss which changed into made by way of binarizing the sensing weights. The projection matrix is implemented inside the Sensing Layer and the learned scaling elements are stored in the reconstruction module. The Scaling Layer (ScLa) is followed by way of m Hidden Layers (HLa). Those hidden layers are the use of the rectified linear unit (Relu) activation feature [17]. The output layer (OLa) is a linear completely related layer which has the size is equivalent to the input size. The HLa’s within the reconstruction module is fully connected and observed by a batch-normalization layer [18]. 
S2V 
IM ILa SeLa ScLa HLa1 HLa 2 HLa m OLa 
Θabx 
S2 
S 
HLa1 
HaL2 
HLam 
S2 
S2V 
S 
… 
FIGURE1. STDAENN Architecture 
3.2 STDAENN Process 
In general, network training uses the standard mini-batch gradient descent method. zi, z^i denotes the input and reconstructed patches, respectively, with zi, z^i ϵ Rn, n = S2. The MSE is obtained as the loss function for the difference between the input and the reconstruction 66 D.M. ANNIE BRIGHTY CHRISTLIN, M. SAFISH MARY 

N 
L= 1 Σ || zi- z^i||22 (4) 
N i=1 
Here N is the no. of pattern patches. 
Algorithm: 
1. A sparsifying and binarization steps are delivered on the sensing layer. 
2. The continuous-valued sensing weights Θ ϵ Rmxn are sparsified to get the sparse weights Θax ϵ Rmxn. 
3. For this step, two among the sparse weight Θax retain only the entries that correspond to the top-K biggest absolute values as Θ and set all the remaining entries to zero. This technique is termed as a K-Select function. The selection of the pinnacle weights can be assigned column-wise, row-wise or over the whole matrix. However, because the scaling layer replaces column-wise operator on the sensing weights, the selection from the top K entries can be made in a column-wise manner. 
4. In the sensing layer, every neuron are connected to K = S2γ factors as the input signal, where γ is the sparsity ratio. 
5. A sparse binary mask Ma ϵ {0,1} m x n is built with entries equal to 1 that corresponds to the largest weights in Θ implementation wise. 
6. The sparse sensing weights are updated in accordance with Θax = Ma ʘa Θ the place ʘa represents the Hadamard product. 
7. The sparse continuous-valued weights Θax ϵ R n x m are mapped to the sparse binary weights Θabxϵ {-1,0,+1} m x n. This process is required for binarization. However, both the sparse and binarization step creates some loss that can be recovered in the process of reconstruction. 
8. The sparse binarized weights Θabx are mapped inversely to the continuous weights Θ using the layer weights αx ϵ Rm . Thus, the output of the scaling layer αx ΘabxTy is an approximation of ΘTy, which corresponds to the non-stop projections ΘT. 
67 SPARSE REPRESENTATION USING COMPRESSED SENSING VIA DEEP LEARNING 

Mathematical Representation: 
Let θ (j) and θabx(j) are the jth columns of Θ and Θabx, respectively. θ(j) corresponds to the dense non-stop weights of the jth hidden units in the sensing layer. Let approximate θ (j) with θ abx(j), the place αx ϵ V+is a scale factor, corresponding to the jth entry of the scaling weights αx. The values of θabx(j) and θ(j) can be determined by minimizing the following imply rectangular error for θabx(j),αx(j) 
E= || θ(j)- αx(j) θabx(j)||22 (5) 
By expanding (5), we have: 
E= θ(j)T θ(j) - 2 αx(j) θ(j)Tθabx(j) + αx(j)2 θ abx(j)Tθabx(j)(6) 
Where θ(j)T and θ(j) are constant. As αx(j) is taken as the sparsity constraint[15], the optimal sparse binary vector θabx(j) is obtained as a solution to the problem is given below: 
θabx(j)*= argmax(θ(j)Tθabx(j))θabx(j) 
such that θabx(j) ϵ {-1,0,+1}m xn 
supp(θabx(j))=supp(θax(j)) (7) 
whereθ ax(j) is the jth column of Θsx, and supp ʘ denotes the nonzero entrie’s position in the treated vector. The result of (7) is a vector , which contains the sign of θabx(j). After obtaining the optimal θabx(j), the optimal θ(j) is obtained by making the derivative of eqn equal to zero. 
Consider θ abx(j)T, θabx(j)= K, 
the optimal α(j) is given by: 
α(j)= 1/K (θ(j)Tθabx(j))= 1/K (Σi=1n θ(ji)θabx(ji) = 1/K (||θ ax(j) ||1) (8) 
After the completion of sparsifying and binarization steps, the resultant Θabx is sparse , which has the K nonzero entries{-1, +1}in each of its columns. During the forward and backward propagation, the sparse binary weights Θabx are used This process is done after this algorithm works with binary weights [13,14,15]. An excessive precision weights Θ are used for making the small modifications of the weights after every update step through the replacement of the parameter Wt,. In the proposed training, Θ is updated by the gradient of loss characteristic for Θabx. This replacement of parameter is referred to as an Adam parameter update. Θabx contains only discrete 68 D.M. ANNIE BRIGHTY CHRISTLIN, M. SAFISH MARY 

weights such as {-1,+1} during the gradient of loss which is concerned still lies in the continuous domain. 
Finally, the loss, newly updated weights of continuous and reconstruction domain such as Θt, Wt the sparse binary sensing weight Θabxt and the scaling layer weight αxt are obtained. 
The overall performance of the proposed approach is in contrast with the existing techniques primarily based on the PSNR using a sparse ternary projection to take a look at the image quality. 
4. EXPERIMENTAL RESULT ANALYSIS 
The ILSVRC2012 set [19] contains 2000 samples of 50 K images are taken for training this network. The proposed model is examined on the two checking out set of 256X256 resolution images. The first testing set consists of 10 images, taken from the ILSVCR2014 [19] dataset, and is given by using the authors of [7]. The second testing set is composed of 50 images chosen from the LabelMe dataset [20] randomizely. All the images had been converted to gray scale in this experiment. These are run in 32 x 32 dimension pixels of small image patches for reducing the computational overhead. The 2000 patches are sampled randomly to construct the coaching set. In the training time, the input patches have been preprocessed through the well known deviation. This network is skilled the use of the proposed training algorithm with the Adam parameter replace [21] a batch size of 5000, 50 epochs and 0.01 decomposing by means of an aspect of 0.6 each and every 5 epochs. The training samples have been randomly shuffled after each epoch. The l2 regularization is applied on the reconstruction module, with a weight as 0.001 for keeping away from out-of-fitting. During the testing phase, these overlapping patches are sampled from each test image with a stride of 2 pixels and decided the ultimate image reconstruction as the average of the patches’ reconstruction. These strategies are evaluated by PSNR value which is expressed in dB.The variety of non-linear hidden layers are assigned empirically L to 2, each with 2048 hidden devices are referred to this community architecture. So this configuration produces a good outcome between training time and reconstruction quality. 69 SPARSE REPRESENTATION USING COMPRESSED SENSING VIA DEEP LEARNING 

Evaluation of PSNR based on sensing rate 
This network is trained and examined with different sensing rates using γ = 0.1 The small changes are appeared R in [0.1,0.3]. The mean PSNR values on the first testing set are proven in Table-1. So the resultant better reconstruction fine is received with large sensing quotes because greater data from the sign is retained in the measurements. 
Table-1. Reconstruction Performance based on varying 
Sensing rateR (γ=0.1) R 	0.1 	0.15 	0.2 	0.25 	0.3 
PSNR 	30.85 	30.83 	31.0 	31.28 	31.1 

